# -*- coding: utf-8 -*-
"""Part 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cZRudmy6qcDdfL55qB9-R7B2eDQPJ88V

# Read Sub Demographic Data
"""

from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.linalg import Vectors
from pyspark.ml.linalg import VectorUDT
import matplotlib.pyplot as plt
from pyspark.ml.feature import PCA
from pyspark.ml.linalg import DenseVector


spark = SparkSession.builder.appName("my project 2")\
    .config("spark.kryoserializer.buffer.max", "512m")\
    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0')\
        .getOrCreate()
sc = spark.sparkContext


for_students_demo_path = "proj_B_demographic"
demographic_df = spark.read.parquet(for_students_demo_path)
display(demographic_df)

"""# Read Static viewing data from Kafka"""

kafka_server = ""
topic='viewstatic'
OFFSETS_PER_TRIGGER = 50000
SCHEMA = "device_id STRING, event_date STRING, event_time STRING, station_num STRING, prog_code STRING, household_id LONG"

static_df = spark.read\
                  .format("kafka")\
                  .option("kafka.bootstrap.servers", kafka_server)\
                  .option("subscribe", topic)\
                  .option("startingOffsets", "earliest")\
                  .option("failOnDataLoss",False)\
                  .load()

static_view_data = static_df.select(F.from_csv(F.decode("value", "US-ASCII"), schema=SCHEMA).alias("value")).select("value.*").na.drop()
display(static_view_data)

demographic_df.cache()
static_view_data.cache()

"""# Feature Extraction"""

def normalize_feature(df, feature):
    #getting the max from the df[feature]
    max_value = df.select(feature).agg({feature: "max"}).collect()[0][0]
    #normlize the column
    normalized_df = df.withColumn(feature, col(feature) / max_value)
    return normalized_df

def encode_feature(df, feature):
    index_feature = ['index_' + feature] #list of features with index before
    feature_vec = ['vec_' + feature] #list of features with vec_ before
    indexer = StringIndexer(inputCols=[feature], outputCols=index_feature)
    model = indexer.fit(df)
    output = model.transform(df)
    encoder = OneHotEncoder(inputCols=index_feature, outputCols=feature_vec)
    model = encoder.fit(output)
    output = model.transform(output)
    return output

type_conversion = {
    'household_size': DoubleType(),
    'num_adults': DoubleType(),
    'num_generations': DoubleType(),
    'length_residence': DoubleType(),
    'home_market_value': DoubleType(),
    'net_worth': DoubleType(),
    'education_highest': DoubleType()
}
#casting the types
for column, data_type in type_conversion.items():
    demographic_df = demographic_df.withColumn(column, F.col(column).cast(data_type))

#features to normlize
numeric_features = ['household_size','num_adults',\
                        'num_generations','length_residence','home_market_value','net_worth','education_highest']

#features to encode
for feature in numeric_features:
    demographic_df = normalize_feature(demographic_df, feature)


#features to one hot encoding
categorical_features = ['marital_status','race_code','dwelling_type','home_owner_status','gender_individual']
index_features = [feature +'_index' for feature in categorical_features]
features_vec = [feature +'_vec' for feature in categorical_features]

#encode the features
for feature in categorical_features:
    demographic_df = encode_feature(demographic_df, feature)

#list of all features
all_features = ['vec_' + feature for feature in categorical_features]+[feature for feature in numeric_features]
assembler = VectorAssembler(
    inputCols= all_features,
    outputCol="features")

output = assembler.transform(demographic_df)
demographic_df = output
demographic_df.select('household_id','features').show(7 , truncate=False)

"""# Visual Analysis"""

#PCA
pca = PCA(k=2, inputCol="features", outputCol="pcaFeatures")
pcaModel = pca.fit(demographic_df)
pca_df = pcaModel.transform(demographic_df)

pandas_df = pca_df.toPandas()
pandas_df['pca_x'] = pandas_df['pcaFeatures'].apply(lambda v: v[0])
pandas_df['pca_y'] = pandas_df['pcaFeatures'].apply(lambda v: v[1])


# Plot the scatter plot
pandas_df.plot.scatter(x='pca_x', y='pca_y',figsize= (14,10))
plt.title("PCA Scatter Plot")
plt.show()

pca_df.select("household_id", "pcaFeatures").show(7,truncate=False)

"""### We can see 6 clusters from the plot

# Clustering
"""

from pyspark.ml.clustering import KMeans
from pyspark.sql.functions import col
from pyspark.sql.window import Window

demographic_df=demographic_df.select('household_id','features')
demographic_df.persist()
k = 6
kmeans = KMeans(featuresCol='features').setK(k).setSeed(3)
#fit the K-means model on the data
model = kmeans.fit(demographic_df)
features_data = demographic_df.select('features')
#make predictions
predictions = model.transform(demographic_df)

# create a mapping of cluster labels to centroids
centroids = model.clusterCenters()
centroid_mapping = {str(idx): centroid.tolist() for idx, centroid in enumerate(centroids)}

# define a UDF to retrieve centroid based on cluster label
get_centroid_udf = udf(lambda label: DenseVector(centroid_mapping.get(str(label))), VectorUDT())

# add a column with the centroid for each cluster
df_with_centroid = predictions.withColumn("centroid", get_centroid_udf(col("prediction")))

calculate_distance_udf = udf(lambda features, centroid: float(Vectors.squared_distance(features, centroid)), DoubleType())
df_with_distance = df_with_centroid.withColumn("distance", calculate_distance_udf(col("features"), col("centroid")))
df_with_distance.select("household_id","prediction","distance").show(7,truncate = False)

"""# Dividing households into subsets"""

#create the window
windowSpec = Window.partitionBy("prediction").orderBy("distance")
#calculate the distances
df_with_distance = df_with_distance.withColumn("row_number", row_number().over(windowSpec))
df_ordered = df_with_distance.orderBy("prediction", "row_number")
df_3rds_subset = df_ordered.filter((col("row_number")) % 3 == 0)
df_17ths_subset = df_ordered.filter((col("row_number")) % 17 == 0)
full_data = df_ordered

#create the subsets for each cluster
subsets = []
k = 6
#get the subsets from the df_3rds
for i in range(0,k):
    #add the subset from 3rds subset
    df_filtered = df_3rds_subset.filter(col("prediction") == i)
    subsets.append(df_filtered)

    #add the subset from 17ths subset
    df_filtered = df_17ths_subset.filter(col("prediction") == i)
    subsets.append(df_filtered)

    #add the subset from full subset
    df_filtered = full_data.filter(col("prediction") == i)
    subsets.append(df_filtered)

#function that get a batch of data and calculate the top 7 stations by diff rank in descending order
def get_top_7(df,batch):
    household_predictions = df.select("household_id","prediction")
    viewing_per_cluster = batch.join(household_predictions,on='household_id')
    total_views = viewing_per_cluster.groupBy("prediction").agg(sum("total_views").alias("total_all_views")).orderBy('prediction')
    viewing_per_station = viewing_per_cluster.groupBy("prediction", "station_num").agg(sum("total_views").alias("total_cluster_station")).join(total_views,on="prediction")
    precent_viewing_event = viewing_per_station.withColumn('precentage',(col("total_cluster_station")/col("total_all_views"))*100).select("prediction",'station_num','precentage')
    total_views = batch.agg(sum('total_views')).collect()[0][0]
    popolarity_rate = batch.groupBy("station_num").agg(sum("total_views").alias("total_station_views")).withColumn("general_poplarity_rating",(col("total_station_views")/total_views)*100).select("station_num","general_poplarity_rating").withColumnRenamed("station_num",'station_number')
    diff_rank = precent_viewing_event.join(popolarity_rate,precent_viewing_event.station_num == popolarity_rate.station_number,'inner' ).withColumn('diff_rank',col("precentage")-col("general_poplarity_rating"))
    df_ordered_desc = diff_rank.orderBy(desc("diff_rank"))
    df_ordered_desc.select("station_num","diff_rank").show(7,truncate = False)

#function that output the get top 7 function according to the instructions
def top7_highest_diff_rank(subsets):
    static_view_data_ag = static_view_data.groupBy("station_num","household_id").count().withColumnRenamed('count','total_views')
    cluster_mapping = {0: "cluster 1", 1: "cluster 2", 2: "cluster 3", 3: "cluster 4", 4: "cluster 5", 5: "cluster 6"}
    subset_mapping = {0: "3rds subset", 1: "17ths subset", 2: "full subset"}
    #for each subset print the top 7 stations
    for cluster_idx, subset_idx in enumerate(range(0, len(subsets), 3)):
        print(cluster_mapping[cluster_idx] + ":")
        for subset_idx_offset in range(3):
            subset = subsets[subset_idx + subset_idx_offset]
            print(subset_mapping[subset_idx_offset])
            get_top_7(subset,static_view_data_ag)
        print("----------------------")

top7_highest_diff_rank(subsets)

"""# Read Streaming viewing data from Kafka"""

topic = "viewstream"
streaming_df = spark.readStream\
                  .format("kafka")\
                  .option("kafka.bootstrap.servers", kafka_server)\
                  .option("subscribe", topic)\
                  .option("startingOffsets", "earliest")\
                  .option("failOnDataLoss",False)\
                  .option("maxOffsetsPerTrigger", OFFSETS_PER_TRIGGER )\
                  .load()\
                  .select(F.from_csv(F.decode("value", "US-ASCII"), schema=SCHEMA).alias("value")).select("value.*")

#get only the df_3rds subsets to each cluster according to the instructions
subsets = []
k = 6
for i in range(0,k):
    #add the subset from 3rds subset
    df_filtered = df_3rds_subset.filter(col("prediction") == i)
    subsets.append(df_filtered)

topic = "viewstream"
streaming_df = spark.readStream\
                  .format("kafka")\
                  .option("kafka.bootstrap.servers", kafka_server)\
                  .option("subscribe", topic)\
                  .option("startingOffsets", "earliest")\
                  .option("failOnDataLoss",False)\
                  .option("maxOffsetsPerTrigger", OFFSETS_PER_TRIGGER )\
                  .load()\
                  .select(F.from_csv(F.decode("value", "US-ASCII"), schema=SCHEMA).alias("value")).select("value.*")

count_events_per_station = streaming_df.groupBy("station_num",'household_id').count().withColumnRenamed('count','total_views')
count_events_query = count_events_per_station.writeStream \
    .queryName('num_events') \
    .format("memory") \
    .outputMode("complete") \
    .start()

from IPython.display import display, clear_output
import time
# Waiting for stream to initialize...
time.sleep(45)
# Starting to access the data stream
for i in range(6):
    print(f"Iteration: {i}")
    print(count_events_query.status)
    batch = spark.sql('SELECT * FROM num_events')
    #use the function get_top_7 to calculate the top stations
    for j in range(len(subsets)):
        print(f"Cluster number {j+1} 3'rd subset diff_rank:")
        get_top_7(subsets[j],batch)
    time.sleep(2)

count_events_query.stop()